# ğŸ› ï¸ Corrective RAG

This project implements a **Corrective Retrieval-Augmented Generation (RAG)** pipeline using **LangGraph**, **Streamlit**, and **Groq LLMs**.  
It demonstrates how to build an **intelligent RAG system** that retrieves, grades, and refines context before generating answers â€” improving reliability compared to standard RAG.

---

## ğŸš€ Features
- ğŸ“„ **Document Retrieval & Grading** â€“ Fetches documents and filters them based on relevance.  
- ğŸ”„ **Query Transformation** â€“ Automatically rewrites questions if retrieved documents are irrelevant.  
- ğŸŒ **Web Search Fallback** â€“ Uses external search when knowledge base documents are insufficient.  
- ğŸ¤– **LLM-Powered Answer Generation** â€“ Uses Groq-hosted LLMs for efficient response generation.  
- ğŸ“Š **Execution Logs in UI** â€“ Displays workflow steps (retrieve â†’ grade â†’ transform â†’ search â†’ generate) directly in the frontend.  

---

## ğŸ“‚ Project Structure

CorrectiveRAG/                # Root project folder
â”œâ”€â”€ app.py                    # Streamlit app entrypoint
â”œâ”€â”€ main.py                   # CLI stub for graph testing
â”œâ”€â”€ requirements.txt          # Project dependencies
â”œâ”€â”€ README.md                 # Documentation
â”œâ”€â”€ .env.example              # Example environment variables
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ langgraphCorrectiveAI/    # LangGraph corrective RAG workflows
â”‚   â”‚   â””â”€â”€ graph/workflow.py     # Core workflow (retrieve â†’ grade â†’ transform â†’ web search â†’ generate)
â”‚   â”‚
â”‚   â”œâ”€â”€ nodes/                    # Modular workflow nodes
â”‚   â”‚   â”œâ”€â”€ retrieve_node.py
â”‚   â”‚   â”œâ”€â”€ grade_node.py
â”‚   â”‚   â”œâ”€â”€ transform_node.py
â”‚   â”‚   â”œâ”€â”€ web_search_node.py
â”‚   â”‚   â””â”€â”€ generate_node.py
â”‚   â”‚
â”‚   â”œâ”€â”€ tools/                    # Tools (retrievers, graders, web search utilities)
â”‚   â”‚   â””â”€â”€ search_tool.py
â”‚   â”‚
â”‚   â””â”€â”€ state/
â”‚       â””â”€â”€ graph_state.py        # Shared state across workflow execution
â”‚
â””â”€â”€ UI/streamlitUI/               # Streamlit-based UI layer
    â”œâ”€â”€ display_result.py
    â”œâ”€â”€ loadui.py
    â””â”€â”€ uiconfigfile.py


---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the Repository
git clone https://github.com/daanyal-23/corrective-rag-demo.git
cd corrective-rag-demo
2ï¸âƒ£ Create a Virtual Environment
python -m venv venv
# On Windows
venv\Scripts\activate
# On Mac/Linux
source venv/bin/activate
3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt
4ï¸âƒ£ Configure Environment Variables
Create a .env file in the root directory:
GROQ_API_KEY=your_groq_api_key
TAVILY_API_KEY=your_tavily_api_key
5ï¸âƒ£ Run the Streamlit App
streamlit run app.py
ğŸ§ª Example Workflow
Enter a question in the Streamlit UI.

System retrieves documents and checks for relevance.

If irrelevant, the query is rewritten and a web search is performed.

The final LLM-generated answer is displayed.

The execution steps are shown in the UI for transparency.

ğŸ“Œ Future Improvements
âœ… Add unit tests for workflow nodes.

âœ… Improve frontend visualization of execution steps.

âœ… Extend support for multiple vector stores (e.g., FAISS, Pinecone).

âœ… Containerize using Docker for deployment.

ğŸ¤ Contributing
Pull requests are welcome!
For major changes, please open an issue first to discuss what you would like to improve.

Made with â¤ï¸ using LangGraph + Groq + Streamlit


